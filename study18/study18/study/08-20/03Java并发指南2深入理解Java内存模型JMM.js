一：'JMM基础与happens-before'
'并发编程模型'的'分类'
在'并发编程中'，我们需'要处理''两个关键问题'：
'线程之间如何通信'及'线程之间如何同步'（'这里的线程''是'指'并发执行的活动实体'）。
'通信'是'指线程之间''以何种机制'来'交换信息'。
在'命令式编程中'，线程之间的'通信机制有两种'：'共享内存'和'消息传递'。

在'共享内存并发模型里'，'线程之间共享程序''的公共状态'，
'线程之间通过''写-读''内存中的公共状态'来'隐式进行通信'。'在消息传递的并发模型'里，
'线程之间''没有公共状态'，'线程之间'必'须通过明确的发送消息'来'显式进行通信'。

'同步是指''程序用于控制''不同线程之间操作发生''相对顺序''的机制'。
在'共享内存并发模型里'，'同步是显式''进行的'。
'程序员'必'须显式指定'某个'方法'或某段'代码'需要'在线程之间互斥执行'。
在'消息传递并发模型里'，由于'消息的发送'必须'在消息的接收之前'，因此'同步是隐式进行的'。

'Java内存模型的抽象'
Java的'并发''采用的是''共享内存模型'，'Java线程'之间的'通信'总'是隐式进行'，
整个'通信过程''对程序员''完全透明'。
如果'编写多线程程序'的Java程序员'不理解隐式进行的线程之间通信''的工作机制'，
很可能'会遇到''各种奇怪的内存可见性问题'。

在java中，'所有实例域'、'静态域'和'数组元素''存储在堆内存中'，
'堆内存''在线程之间共享'（'本文使用“共享变量”'这个术语代'指实例域'，'静态域'和'数组元素'）。
'局部变量'（Local variables），'方法内定义的参数'（java语言规范称之为formal method parameters）
'和异常处理器参数'（exception handler parameters）'不会在线程之间共享'，
它们'不会有内存可见性问题'，也'不受内存模型的影响'。

'Java线程'之间的'通信''由Java内存模型'（本文简称为JMM）'控制'，
'JMM''决定一个线程''对共享变量的写入''何时对另一个线程可见'。从'抽象'的角度'来看'，
'JMM''定义了线程和主内存之间''的抽象关系'：'线程之间的共享变量''存储在主内存（main memory）中'，
'每个线程'都'有一个私有的本地内存'（local memory），
'本地内存'中'存储了该线程'以'读/写共享变量的副本'。
'本地内存''是''JMM的一个抽象概念'，'并不真实存在'。
'它涵盖'了'缓存，写缓冲区，寄存器'以'及其他'的'硬件和编译器优化'。
Java内存模型的抽象示意图如下：

'从上图来看'，'线程A与线程B'之间'如要通信'的话，必须'要经历'下面'2个步骤'：

首先，'线程A把本地内存A'中'更新过的共享变量''刷新到主内存'中去。
然后，'线程B到主内存中去读取线程A'之前已'更新过的共享变量'。
下面'通过示意图'来'说明这两个步骤'：

如上图所示，'本地内存A和B''有主内存中''共享变量x的副本'。
假设初始时，这'三个内存中'的'x值都为0'。
线程'A在执行时'，'把更新后的x值'（假设值为1）临时'存放在自己的本地内存A中'。
'当线程A和线程B需要通信时'，线程'A首先会把自己本地内存'中'修改后的x值刷新到主内存'中，
此时'主内存中的x值变为了1'。随后，'线程B到主内存'中'去读取线程A更新后的x值'，
此时'线程B的本地内存的x值也变为了1'。

从'整体来看'，这'两个步骤'实质上'是线程A在向线程B发送消息'，
而且'这个通信过程'必须'要经过主内存'。
'JMM''通过控制主内存''与每个线程的本地内存'之间'的交互'，来'为java程序员''提供内存可见性保证'。

'重排序'
在'执行程序时''为'了'提高性能'，'编译器和处理器'常常'会对指令做重排序'。
'重排序''分三种类型'：
'编译器优化'的'重排序'。'编译器'在'不改变单线程程序语义'的'前提下'，可以'重新安排语句'的'执行顺序'。

'指令级并行'的'重排序'。'现代处理器''采用了''指令级并行技术'（Instruction-Level Parallelism， ILP）
来'将多条指令''重叠执行'。'如果不存在数据依赖性'，'处理器可以改变语句''对应机器指令的执行顺序'。

'内存系统'的'重排序'。'由于处理器使用缓存''和读/写缓冲区'，
这'使得加载和存储操作'看上去'可能是在乱序执行'。

从'java源代码''到最终实际执行的指令序列'，'会分别经历下面三种重排序'：

上述的'1属于编译器''重排序'，'2和3属于处理器''重排序'。
这些重排序都可能'会导致多线程程序'出现'内存可见性问题'。
对于编译器重排序，'JMM'的'编译器重排序'规则'会禁止''特定类型的编译器''重排序'
（'不是所有的编译器''重排序''都要禁止'）。
对于处理器重排序，'JMM'的'处理器重排'序规则'会要求java编译器'在'生成指令序列时'，
'插入特定类型'的'内存屏障'（memory barriers，intel称之为memory fence）指令，
'通过内存屏障指令'来'禁止特定类型的处理器''重排序'（'不是所有的处理器''重排序都要禁止'）。

'JMM''属于语言级'的'内存模型'，它'确保在不同的编译器'和'不同的处理器平台'之上，
'通过禁止特定类型'的'编译器重排序'和'处理器重排序'，为程序员'提供一致'的'内存可见性保证'。

'处理器重排序'与'内存屏障指令'
'现代的处理器使用''写缓冲区'来'临时保存向内存写入的数据'。
'写缓冲区''可以保证指令流水线''持续运行'，
'它可以避免'由于'处理器停顿下来''等待向内存写入数据'而'产生的延迟'。
'同时'，'通过''以批处理的方式''刷新写缓冲区'，'以及''合并写缓冲区'中'对同一内存地址的多次写'，
'可以减少对内存总线的占用'。虽然'写缓冲区有这么多好处'，但'每个处理器'上'的写缓冲区'，
'仅仅对它所在的处理器''可见'。'这个特性'会'对内存操作的执行顺序''产生重要的影响'：
'处理器对内存'的'读/写操作'的'执行顺序'，'不一定与内存实际发生'的'读/写操作顺序一致'！
为了具体说明，请看下面示例：
Processor A	 Processor B
a = 1; 'A1'  b = 2; 'B1'
x = b; 'A2'  y = a; 'B2'
'初始状态：a = b = 0'
'处理器'允许'执行后'得到'结果：x = y = 0'

'假设''处理器A'和'处理器B''按程序的顺序'并行'执行内存访问'，最终'却可能得到x = y = 0的结果'。
具体的'原因如下图所示'：

'这里处理器A'和'处理器B''可以同时''把共享变量''写入自己的写缓冲区'（A1，B1），
然后'从内存中读取''另一个共享变量（A2，B2）'，
'最后'才'把自己写缓存区中保存的脏数据''刷新到内存中（A3，B3）'。
当以'这种时序执行时'，程序就'可以得到x = y = 0的结果'。

'从内存操作''实际'发生的'顺序来看'，直到'处理器A执行A3'来'刷新自己的写缓存区'，
'写操作A1才算真正执行了'。虽然'处理器A执行内存操作的顺序为：A1->A2'，
'但内存操作实际发生的顺序'却'是：A2->A1'。
此时，'处理器A的内存操作''顺序被重排序了'（处理器B的情况和处理器A一样，这里就不赘述了）。

这里的'关键是'，由于'写缓冲区''仅对自己的处理器可见'，
'它会导致''处理器执行内存操作的顺序''可能会''与内存实际的操作执行顺序''不一致'。
'由于现代的处理器''都会使用写缓冲区'，'因此''现代的处理器'都'会允许''对写-读操做重排序'。

下面是'常见处理器''允许的重排序类型'的'列表'：

 		Load-Load	Load-Store	Store-Store	Store-Load	数据依赖
sparc-TSO	N			N			N			Y			N
x86			N			N			N			Y			N
ia64		Y			Y			Y			Y			N
PowerPC		Y			Y			Y			Y			N

上表单元格中的“'N'”'表示'处理器'不允许'两个操作'重排序'，“'Y'”'表示允许''重排序'。

从'上表'我们'可以看出'：'常见'的'处理器''都允许Store-Load''重排序'；
常见的处理器'都不允许'对'存在数据依赖的操作'做'重排序'。
'sparc-TSO和x86'拥'有相对较强'的'处理器内存模型'，
'它们仅允许'对'写-读操作''做重排序'（因为它们'都使用了写缓冲区'）。

※注1：sparc-TSO是指以TSO(Total Store Order)内存模型运行时，sparc处理器的特性。

※注2：上表中的'x86包括x64及AMD64'。

※注3：由于ARM处理器的内存模型与PowerPC处理器的内存模型非常类似，本文将忽略它。

※注4：数据依赖性后文会专门说明。

'为了保证''内存可见性'，
'java编译器''在生成指令序列的''适当位置''会''插入内存屏障指令'来'禁止特定类型'的'处理器重排序'。
'JMM''把内存屏障指令''分为下列四类'：

屏障类型					指令示例				说明
'LoadLoad' Barriers			Load1;LoadLoad; Load2【''】
'确保Load1数据装载'，之前于'Load2''及'所有'后续装载指令''的装载'。

'StoreStore' Barriers		Store1; StoreStore; Store2【''】	
'确保Store1数据''对其他处理器''可见'（刷新到内存），'之前于Store2''及所有后续存储指令''的存储'。

'LoadStore' Barriers		Load1; LoadStore; Store2【''】
'确保Load1数据装载'，之'前于Store2''及'所有'后续的存储指令''先刷新到内存'。

'StoreLoad' Barriers		Store1; StoreLoad; Load2【'Store1先刷到内存'】
'确保Store1数据''对其他处理器''变得可见'（指刷新到内存），之'前于所有装载指令''的装载'。
'StoreLoad' Barriers会'使'该'屏障之前'的所有内存'访问指令'（存储和装载指令）'完成之后'，
'才执行'该'屏障之后'的内存'访问指令'。

'StoreLoad' Barriers'是一个“全能型”的屏障'，它'同时具有''其他三个屏障''的效果'。
'现代的多处理器'大都'支持该屏障'（'其他类型的屏障''不一定''被所有处理器支持'）。
'执行该屏障''开销'会'很昂贵'，因为'当前处理器'通常要'把写缓冲区中的数据'全部'刷新到内存中'（buffer fully flush）。

'happens-before'
从'JDK5开始'，'java'使'用新的JSR-133内存模型'（本文除非特别说明，针对的都是JSR- 133内存模型）。
'JSR-133''提出'了'happens-before的概念'，通过这个概念'来阐述操作之间'的'内存可见性'。
如果一个'操作执行'的'结果'需'要对另一个操作''可见'，那么这'两个操作''之间'必须'存在happens-before关系'。
这里提到的'两个操作''既可以'是'在一个线程之内'，'也'可以是'在不同线程之间'。 

与程序员密切相关的'happens-before规则如下'：
'程序顺序规则'：'一个线程'中的'每个操作'，'happens- before于''该线程中'的'任意后续操作'。
'监视器锁规则'：'对一个监视器锁''的解锁'，'happens- before于''随后对这个监视器锁''的加锁'。
'volatile变量规则'：对'一个volatile域的写'，'happens- before于'任意'后续对这个volatile域''的读'。
'传递性'：'如果A happens- before B'，'且B happens- before C'，'那么A happens- before C'。

'注意'，'两个操作'之间'具有happens-before关系'，并'不意味'着'前一个操作'必须'要在后一个操作'之'前执行'！
'happens-before'仅仅'要求前一个操作'（执行的结果）'对后一个操作''可见'，
'且前一个操作''按顺序''排在第二个操作之前'（the first is visible to and ordered before the second）。
happens-before的定义很微妙，后文会具体说明happens-before为什么要这么定义。

'happens-before与JMM'的'关系如下图'所示：

'如上图所示'，一个'happens-before规则'通常'对应于多个编译器''重排序规则'和'处理器重排序规则'。
对于java程序员来说，'happens-before规则''简单易懂'，
它'避免程序员''为了理解''JMM'提供的'内存可见性保证'而'去学习复杂的重排序规则'以'及这些规则的具体实现'。

二：'重排序''与JMM'的'as-if-serial'

'数据依赖性'
如果'两个操作访问''同一'个'变量'，且这'两个操作'中'有一个为写操作'，
此时'这两个操作'之间'就存在数据依赖性'。'数据依赖''分''下列三种类型'：

名称	代码示例	说明
'写后读'	a = 1;b = a;	写一个变量之后，再读这个位置。
'写后写'	a = 1;a = 2;	写一个变量之后，再写这个变量。
'读后写'	a = b;b = 1;	读一个变量之后，再写这个变量。
上面三种情况，'只要重排序两个操作'的'执行顺序'，程序的'执行结果'将'会被改变'。


注意，这里所说的'数据依赖性'仅'针对单个处理器'中'执行的指令序列'和'单个线程'中'执行的操作'，
'不同处理器'之间和'不同线程'之间的'数据依赖性''不被编译器'和'处理器考虑'。
前面提到过，'编译器和处理器'可能'会对操作''做重排序'。
'编译器和处理器'在'重排序时'，会'遵守数据依赖性'，
'编译器和处理器'不会'改变存在数据依赖关系'的'两个操作'的'执行顺序'。

'as-if-serial语义'
as-if-serial'语义的意思'指：'不管怎么重排序'（编译器和处理器为了提高并行度），
（单线程）'程序的执行结果''不能被改变'。
'编译器，runtime 和处理器'都必须'遵守as-if-serial语义'。

'为'了'遵守as-if-serial语义'，'编译器和处理器''不会''对存在数据依赖关系''的操作做重排序'，
因为'这种重排序''会改变执行结果'。但是，如果'操作之间不存在数据依赖关系'，
这些'操作可能被编译器'和'处理器重排序'。
为了具体说明，请看下面计算圆面积的代码示例：
double pi  	= 3.14;		//A
double r   	= 1.0;		//B
double area = pi*r*r;	//C
上面三个操作的数据依赖关系如下图所示：

如上图所示，'A和C'之间'存在数据依赖关系'，同时'B和C'之间'也存在数据依赖关系'。
因此在'最终执行的指令序列中'，'C不能被重排序''到A和B的前面'
（'C排到A和B的前面'，程序的'结果将会被改变'）。但'A和B'之间'没有数据依赖关系'，
编译器和处理器'可以重排序A和B之间的执行顺序'。下图是该程序的两种执行顺序：

'as-if-serial语义''把单线程程序''保护了起来'，'遵守as-if-serial语义'的'编译器'，
'runtime和处理器'共同'为''编写单线程程序''的程序员''创建'了一个'幻觉'：
'幻觉-单线程程序是按程序的顺序来执行的'。
'as-if-serial语义''使单线程程序员''无需担心重排序'会干扰他们，也'无需担心内存可见性问题'。

'程序顺序规则'
根据'happens- before'的'程序顺序规则'，上面计算'圆的面积的示例代码''存在三个happens-before'关系：
A happens- before B；
B happens- before C；
A happens- before C；
这里的第3个happens- before关系，是根据happens- before的传递性推导出来的。

这里'A happens-before B'，但'实际执行时''B却可以排在A之前执行'（看上面的重排序后的执行顺序）。
在第一章提到过，'如果A happens-before B'，'JMM并不要求A一定要在B之前执行'。
'JMM''仅仅要求前一个操作'（执行的结果）'对后一个操作可见'，'且前一个操作''按顺序排在第二个操作之前'。
这里'操作A的执行结果''不需要对操作B可见'；而且'重排序操作A和操作B后的执行结果'，
与'操作A和操作B按happens- before顺序执行的结果一致'。
在这种情况下，'JMM'会认为这种重排序并不非法（not illegal），JMM'允许这种重排序'。

在'计算机中'，'软件'技术和'硬件'技术有一个'共同的目标'：在'不改变''程序执行结果'的'前提下'，
'尽可能'的'开发并行度'。'编译器和处理器''遵从这一目标'，'从happens- before'的'定义'我们'可以看出'，
'JMM''同样遵从这一目标'。

'重排序''对多线程''的影响'
现在让我们来看看，'重排序'是否'会改变多线程程序'的'执行结果'。请看下面的示例代码：
class ReorderExample {
	int a = 0;
	boolean flag = false;
	 
	public void writer() {
	    a = 1;                   //1
	    flag = true;             //2
	}
	 
	Public void reader() {
	    if (flag) {              //3
	        int i =  a * a;      //4
	        ……
	    }
	}
}
'flag变量''是个标记'，用来'标识变量a''是否''已被写入'。这里'假设有两个线程A和B'，
'A首先执行writer()方法'，随后'B线程''接着执行reader()方法'。'线程B在执行操作4时'，
'能否看到线程A'在'操作1''对共享变量a的写入？'
答案是：'不一定能看到'。

由于'操作1和操作2''没有数据依赖关系'，'编译器和处理器'可以'对这两个操作重排序'；
同样，操作3和操作4没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。
让我们先来看看，当操作1和操作2重排序时，可能会产生什么效果？请看下面的程序执行时序图：


如上图所示，'操作1和操作2做了重排序'。程序执行时，'线程A首先写标记变量flag'，
随后'线程B读这个变量'。由于条件判断为真，'线程B将读取变量a'。
此时，'变量a还根本没有被线程A写入'，在这里'多线程程序的语义''被重排序破坏了！'

※注：本文统一用红色的虚箭线表示错误的读操作，用绿色的虚箭线表示正确的读操作。

下面再让我们看看，
当'操作3和操作4重排序'时'会产生什么效果'（借助这个重排序，可以顺便说明控制依赖性）。
下面是操作3和操作4重排序后，程序的执行时序图：


在程序中，'操作3和操作4''存在控制依赖关系'。
当'代码中存在控制依赖性时'，'会影响指令序列执行的并行度'。
为此，'编译器和处理器'会采'用猜测（Speculation）执行'来'克服控制相关性''对并行度的影响'。
以'处理器的猜测执行为例'，'执行线程B的处理器'可以'提前读取并计算a*a'，
然后'把计算结果'临时'保存到'一个名为'重排序缓冲'（reorder buffer ROB）'的硬件缓存中'。
当接下来'操作3的条件判断为真时'，就'把该计算结果写入变量i中'。

从图中我们可以看出，'猜测执行''实质上对操作3和4做了重排序'。
重排序在这里'破坏了多线程程序的语义！'

在'单线程程序中'，对'存在控制依赖'的'操作重排序'，
'不会改变执行结果'（这也是'as-if-serial语义''允许'对存在'控制依赖的操作'做'重排序'的原因）；
但'在多线程程序中'，对存在'控制依赖的操作重排序'，可能'会改变程序的执行结果'。

三：'顺序一致性内存模型''与JMM'
'数据竞争''与顺序一致性保证'
'当程序未正确同步时'，就'会存在数据竞争'。'java内存模型规范''对数据竞争'的'定义如下'：

在'一个线程中''写一个变量'，
在'另一个线程''读同一个变量'，
而且'写和读''没有通过同步'来'排序'。
当'代码中包含数据竞争'时，'程序的执行'往往'产生违反直觉的结果'（前一章的示例正是如此）。
如果'一个多线程程序'能'正确同步'，'这个程序'将'是一个没有数据竞争的程序'。

'JMM''对正确同步的多线程程序''的内存一致性''做了如下保证'：

如果'程序是正确同步的'，'程序的执行'将具'有顺序一致性'（sequentially consistent）
--即'程序的执行结果''与该程序'在'顺序一致性内存模型'中'的执行结果相同'
（马上我们将会看到，这对于程序员来说是一个极强的保证）。
这里的'同步'是'指广义上的同步'，'包括'对常用'同步原语'（lock，volatile和final）的正确使用。

'顺序一致性内存模型'
'顺序一致性内存模型''是'一个被计算机科学家理想化了的'理论参考模型'，
它为程序员'提供'了极强的'内存可见性保证'。顺序一致性内存模型有两大特性：
'一个线程中'的'所有操作'必'须按照程序的顺序来执行'。
（不管程序是否同步）'所有线程'都'只能看到'一个'单一的操作执行顺序'。
在'顺序一致性内存模型'中，'每个操作'都'必须原子执行'且'立刻对所有线程可见'。
顺序一致性内存模型为程序员提供的视图如下：

在概念上，'顺序一致性模型''有一个''单一的全局内存'，
'这个内存''通过'一个'左右摆动的开关'可以'连接到任意一个线程'。
同时，'每一个线程'必'须按程序的顺序'来'执行内存读/写操作'。
从上图我们可以看出，'在任意时间点''最多只能有一个线程'可以'连接到内存'。
当'多个线程并发执行'时，图中的'开关装置'能'把所有线程'的'所有内存读/写操作串行化'。

为了更好的理解，下面我们'通过两个示意图'来'对顺序一致性模型'的'特性''做进一步的说明'。
'假设有两个线程A和B''并发执行'。其中'A线程有三个操作'，它们在程序中的'顺序是：A1->A2->A3'。
'B线程'也'有三个操作'，它们在程序中的顺序是：'B1->B2->B3'。

'假设这两个线程'使'用监视器来正确同步'：'A线程的三个操作''执行后释放监视器'，
随后'B线程获取同一个监视器'。那么'程序在顺序一致性模型'中的'执行效果'将'如下图所示'：

现在我们再'假设这两个线程''没有'做'同步'，下面是这个'未同步程序'在顺序一致性模型中的'执行示意图'：

'未同步程序'在'顺序一致性模型中'虽然'整体执行顺序是无序的'，
'但所有线程'都'只能看到一个''一致的整体执行顺序'。以上图为例，
'线程A和B''看到的执行'顺序'都是'：B1->A1->A2->B2->A3->B3。
'之所以'能'得到这个保证''是因为''顺序一致性内存模型'中的'每个操作''必须立即对任意线程可见'。

但是，在'JMM中'就'没有这个保证'。'未同步程序''在JMM中''不但整体的执行顺序是无序'的，
'而且''所有线程''看到的操作执行顺序''也可能不一致'。
比如，'在当前线程''把写过的数据''缓存在本地内存'中，且'还没有刷新到主内存之前'，
这个'写操作''仅对当前线程可见'；'从其他线程的角度'来'观察'，
'会认为'这个'写操作'根本'还没有被当前线程执行'。
'只有当前线程''把本地内存中写过的数据''刷新到主内存'之后，这个'写操作''才能对其他线程可见'。
在'这种情况下'，'当前线程'和'其它线程''看到的操作执行顺序''将不一致'。

'同步程序的顺序一致性效果'
下面我们对前面的示例程序ReorderExample'用监视器来同步'，
看看'正确同步的程序''如何具有顺序一致性'。

请看下面的示例代码：

class SynchronizedExample {
	int a = 0;
	boolean flag = false;
	 
	public synchronized void writer() {
	    a = 1;
	    flag = true;
	}
	 
	public synchronized void reader() {
	    if (flag) {
	        int i = a;
	        ……
	    }
	}
}
'上面示例代码中'，'假设A线程''执行writer()方法后'，'B线程执行reader()方法'。
'这是'一个'正确同步'的'多线程程序'。
根据'JMM规范'，'该程序的执行结果'将'与该程序在顺序一致性模型中'的'执行结果相同'。
下面是该程序在两个内存模型中的执行时序对比图：


'在顺序一致性模型中'，'所有操作'完全'按程序的顺序''串行执行'。
而'在JMM中'，'临界区内的代码''可以重排序'
（'但JMM''不允许临界区内的代码'“'逸出'”到临界区之外，'那样会破坏监视器'的'语义'）。
'JMM'会'在''退出监视器'和'进入监视器''这两个关键时间点''做一些特别处理'，
'使线程''在这两个时间点'具'有''与顺序一致性模型相同的''内存视图'（具体细节后文会说明）。

'虽然线程A''在临界区'内'做了重排序'，'但由于监视器'的'互斥执行的特性'，
这里的'线程B'根本'无法“观察”'到'线程A在临界区'内'的重排序'。
'这种重排序''既提高了执行效率'，'又没有改变程序'的'执行结果'。

从这里我们可以看到'JMM''在具体实现上'的'基本方针'：
在'不改变'（正确同步的）'程序执行结果'的'前提下'，
尽可能的'为编译器'和'处理器'的'优化打开方便之门'。

'未同步程序的执行特性'
对于未同步或'未正确同步'的'多线程程序'，'JMM''只提供最小安全性'：
'线程执行时''读取到的值'，'要么是''之前某个线程写入的值'，'要么是默认值'（0，null，false），
'JMM''保证线程读操作''读取到的值''不会无中生有'（out of thin air）的冒出来。
'为了实现最小安全性'，'JVM''在堆上分配对象时'，首先'会清零内存空间'，
'然后'才会'在上面分配对象'（JVM内部会同步这两个操作）。
因此，'在已清零的内存空间'（pre-zeroed memory）'分配对象时'，域的默认初始化已经完成了。

'JMM''不保证''未同步程序'的'执行结果''与该程序'在'顺序一致性模型'中的'执行结果一致'。
因为'未同步程序'在'顺序一致性模型中执行时'，'整体上是无序的'，其'执行结果无法预知'。
'保证未同步程序''在两个模型中的执行结果一致''毫无意义'。

'和顺序一致性模型一样'，'未同步程序''在JMM中的执行时'，'整体上也是无序的'，
其'执行结果'也'无法预知'。同时，'未同步程序''在这两个模型中'的'执行特性''有下面几个差异'：

1'顺序一致性模型''保证单线程'内的'操作'会'按程序的顺序执行'，
而'JMM''不保证单线程内的操作'会'按程序的顺序执行'（比如上面'正确同步的多线程程序''在临界区内'的'重排序'）。
这一点前面已经讲过了，这里就不再赘述。

2'顺序一致性模型''保证所有线程'只'能看到一致'的'操作执行顺序'，
而'JMM''不保证所有线程'能'看到一致的操作执行顺序'。这一点前面也已经讲过，这里就不再赘述。

3'顺序一致性模型''保证对所有的内存读/写操作'都'具有原子性'。
而'JMM''不保证''对64位的long型'和'double型''变量的读/写操作''具有原子性'，
'第3个差异''与处理器总线'的'工作机制密切相关'。
在'计算机中'，'数据通过总线''在处理器'和'内存之间传递'。
'每次处理器'和'内存之间的数据传递'都'是通过一系列步骤'来'完成的'，
'这一系列步骤''称'之'为总线事务'（bus transaction）。

'总线事务''包括读事务'（read transaction）'和写事务'（write transaction）。
'读事务''从内存传送数据''到处理器'，'写事务''从处理器传送数据''到内存'，
'每个事务''会读/写内存中''一个或多个物理'上'连续的字'。
这里的关键是，'总线''会同步''试图并发使用总线''的事务'。
在'一个处理器''执行总线事务期间'，总线'会禁止其它所有的处理器'和'I/O设备'
'执行内存的读/写'。
下面让'我们通过一个示意图'来'说明总线'的'工作机制'：

如上图所示，'假设处理器A，B和C''同时向总线发起总线事务'，
这时'总线仲裁'（bus arbitration）'会对竞争作出裁决'，
这里我们假设总线在仲裁后判定'处理器A''在竞争中''获胜'（总线仲裁会确保所有处理器都能公平的访问内存）。
'此时处理器A''继续''它的总线事务'，
而'其它两个处理器'则'要等待处理器A'的'总线事务完成后''才能开始再次执行''内存访问'。
'假设'在'处理器A执行总线事务期间'（不管这个总线事务是读事务还是写事务），
'处理器D''向总线发起了总线事务'，此时'处理器D'的'这个请求''会被总线禁止'。

'总线的'这些'工作机制''可以把''所有处理器''对内存的访问'以'串行化的方式''来执行'；
在'任意时间点'，'最多只能'有'一个处理器'能'访问内存'。
'这个特性''确保'了'单个总线事务'之中的'内存读/写操作''具有原子性'。

在一些'32位的处理器上'，如果'要求对64位数据'的'读/写操作具有原子性'，'会有比较大的开销'。
为了照顾这种处理器，'java语言'规范'鼓励但不强求''JVM对64位的long型变量'和'double型变量'的'读/写具有原子性'。
当'JVM''在这种处理器上运行时'，
会'把一个64位long/ double型变量'的'读/写操作''拆分为两个32位的''读/写操作''来执行'。
'这两个32位'的'读/写操作'可能'会被分配到不同的总线事务'中'执行'，
此时'对这个64位变量'的'读/写'将'不具有原子性'。

当'单个内存操作''不具有原子性'，将'可能会产生意想不到后果'。请看下面示意图：

如上图所示，'假设处理器A''写一个long型变量'，同时'处理器B'要'读这个long型变量'。
'处理器A'中'64位的写操作''被拆分为''两个32位的写操作'，
且这'两个32位'的'写操作''被分配到''不同的写事务中执行'。
同时'处理器B''中64位的读操作''被拆分为两个32位的读操作'，
且'这两个32位'的'读操作''被分配到''同一个的读事务中执行'。
当处理器A和B按上图的时序来执行时，'处理器B''将看到''仅仅被处理器A“写了一半“''的无效值'。
